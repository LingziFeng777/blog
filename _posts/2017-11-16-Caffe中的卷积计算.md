---
layout: post
title:  Caffe中的卷积计算
date:   2017-11-16
categories: Caffe
tag: caffe,卷积,计算
---

* content
{:toc}


## Caffe的卷积层

在第一次接触Caffe时，阅读了`lenet`的prototxt文件`caffe/example/mnist/lenet_train_test.prototxt`，该文件中描述了`lenet`网络各层的配置信息，其中对于卷积层的定义如下：
```prototxt
layer {    // 定义一个新的卷积层，输入blob为data，输出blob为conv1
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1    // 权值学习速率倍乘因子，1倍表示保持与全局参数一致
  }
  param {
    lr_mult: 2    // bias学习速率倍乘因子，是全局参数的2倍
  }
  convolution_param {    // 卷积计算参数
    num_output: 20       // 输出feature map数目为20，即有20个卷积核
    kernel_size: 5       // 卷积核尺寸，5 X 5
    stride: 1            // 卷积输出跳跃间隔，1表示连续输出，无跳跃
    weight_filler {      // 权值使用xavier填充器
      type: "xavier"
    }
    bias_filler {        // bias使用常数填充器，默认为0
      type: "constant"
    }
  }
}

```

可以看出，Caffe中的`lenet`网络对于卷积层的定义中，只指定了核的大小以及核的个数，上面为别为`5*5`和`20`。对于多通道图像，输出的`feature map`的个数只与卷积核的个数有关，而与通道数无关，比如对于一个单通道的图像进行卷积时，使用`10`个卷积核，得到`10`个`feature map`，当输入为`RGB`三通道时，输出仍为`10`个`feature map`，而不是`30`个，输出的个数依然是卷积核的个数。  

### 1. 最简单的实现: *嵌套循环*

参数定义 ：  

* 图片：宽度为width：`W`，高度为height：`H`，图片的通道数为`D`，一般目前都是`RGB`三通道的`D=3`，但是为了不失一般性用`D`表示。  

* 卷积核：卷积核的大小为`K * K`，每个通道的卷积核其实是不一样的，因此卷积核的大小其实也就是`D * K * K`，设卷积核的个数为`M`。  

```matlab
for w in 1..W
  for h in 1..H
    for x in 1..K
      for y in 1..K
        for m in 1..M
          for d in 1..D
            output(w, h, m) += input(w+x, h+y, d) * filter(m, x, y, d)
          end
        end
      end
    end
  end
end
```

可以看到最内层的循环是针对通道数`D`的，说明对于`D`个通道而言，是在每个通道上分别执行二维卷积，然后将`D`个通道加起来，`output(w, h, m) += input(w+x, h+y, d) * filter(m, x, y, d)`中可以看到输出`output`已经没有了通道的信息，在有`padding`的情况下，能保持输出的特征图片大小和原来一样，总共为`M`个`W * H`的`feature map`。这也是可以理解的，对于一张彩色图像，有`RGB`三个通道，在计算卷积时，同一个位置的三个卷积核分别计算卷积后，它们是可以直接进行叠加的，因为在显示一张图时，每一个像素也是通过对`RGB`三个分量进行简单的叠加而生成的。  

### 2. Caffe里面的计算方式

先贴上JYQ大神的几张图：

<center>
<img src="{{ '/styles/images/jyq1.jpg' | prepend: site.baseurl }}" width="80%" height="80%" />
<img src="{{ '/styles/images/jyq2.jpg' | prepend: site.baseurl }}" width="80%" height="80%" />
<img src="{{ '/styles/images/jyq3.jpg' | prepend: site.baseurl }}" width="80%" height="80%" />
<img src="{{ '/styles/images/jyq4.jpg' | prepend: site.baseurl }}" width="80%" height="80%" />
<img src="{{ '/styles/images/jyq5.jpg' | prepend: site.baseurl }}" width="80%" height="80%" />
</center>

核心思想是将图像数据和卷积核都平铺为二维的矩阵，然后进行矩阵相乘的计算。。先将图像数据根据`filter`的大小以及通道的大小来平铺，`Feature Matrix`的行数为`H*W`，列数为`C*K*K`，这里的`C`为通道数，相当于之前提到的`D`；然后将`filter`进行平铺，行数为`Cout`，列数为`C*K*K`，这里`Cout`为卷积核的个数，相当于之前提到的`M`；最后用`Filter Matrix`乘以`Feature Matrix`的转置，得到输出矩阵`Cout * (H * W)`，即`Cout`个`H*W`的`feature map`。  

### 3. im2col方法
在Caffe的具体实现中，ConvolutionLayer的主要作用就是进行卷积操作，其中比较主要的函数就是im2col和col2im，为了方便理解，下图展示了caffe中具体的优化方法，图中上半部分是一个传统的卷积，下半部分是矩阵相乘的版本。
<center>
<img src="{{ '/styles/images/im2col.jpg' | prepend: site.baseurl }}"/>
</center>

图中可以看出来，图像大小为`3*3`，共有`3`个通道，卷积核的个数`M = 2`，但是由于通道数为`3`，所以实际上的核数量为`6`；从上图中可以看出来矩阵相乘的方式非常方便，并且矩阵计算还方便使用GPU加速。  

### 4. 举个例子
![]({{ '/styles/images/im2coljuli.png' | prepend: site.baseurl }})  


## *参考*  
[1] [http://blog.csdn.net/u014381600/article/details/60883856](http://blog.csdn.net/u014381600/article/details/60883856)  
[2] [http://blog.csdn.net/u014114990/article/details/51125776](http://blog.csdn.net/u014114990/article/details/51125776)  
[3] [https://www.zhihu.com/question/28385679](https://www.zhihu.com/question/28385679) 